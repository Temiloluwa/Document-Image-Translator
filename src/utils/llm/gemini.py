from ._base import LLM
from google import genai
from google.genai.types import GenerateContentConfig, GenerateContentResponse
from models.message import GeminiUserMessage, GeminiAssistantMessage
from PIL import Image
from typing import Dict, Any, List
import os


class Gemini(LLM):
    """
    A class representing the Gemini LLM (Large Language Model) interface.

    This class provides methods to set up and interact with the Gemini LLM for generating content
    based on text prompts and images.
    """

    def __init__(self, system_instruction: str, **kwargs: Dict[str, Any]) -> None:
        """
        Initialize the Gemini LLM with a system instruction and optional configuration parameters.

        Args:
            system_instruction (str): The system instruction to guide the LLM's behavior.
            **kwargs: Additional configuration parameters for the LLM.
        """
        self._setup(system_instruction, **kwargs)

    def _setup(self, system_instruction: str, **kwargs: Dict[str, Any]) -> None:  # type: ignore[override]
        """
        Set up the Gemini LLM client and configuration.

        Args:
            system_instruction (str): The system instruction to guide the LLM's behavior.
            **kwargs: Additional configuration parameters for the LLM.
        """
        self._config = GenerateContentConfig(
            system_instruction=system_instruction,
            **kwargs,  # type: ignore[arg-type]
        )
        self._client = genai.Client(api_key=os.getenv("TRANSLATION_MODEL_API_KEY"))

    async def generate(  # type: ignore[override]
        self,
        prompt: str,
        image: List[Image.Image],
        model: str = "gemini-1.5-flash-001",  # type: ignore[arg-type]
    ) -> GeminiAssistantMessage:
        """
        Generate a response from the Gemini LLM based on a text prompt and an image.

        Args:
            prompt (str): The text prompt to guide the LLM's response.
            images (List[Image.Image]): A list of images to provide additional context for the LLM.

        Returns:
            GeminiAssistantMessage: The response generated by the Gemini LLM.
        """
        prompt = prompt.replace("\n", " ").strip()

        response = await self._client.aio.models.generate_content(
            model=model,  # type: ignore[arg-type]
            contents=GeminiUserMessage(prompt, image).contents,  # type: ignore[arg-type]
            config=self._config,
        )
        return self._parse_output(response)

    def _parse_output(  # type: ignore[override]
        self, response: GenerateContentResponse
    ) -> GeminiAssistantMessage:
        """
        Parse the output from the GenerateContentResponse to extract the generated message.

        Args:
            response (GenerateContentResponse): The response from the Gemini LLM containing the generated content.

        Returns:
            GeminiAssistantMessage: The parsed message from the LLM's response.
        """
        response = response.candidates[0].content.parts[0].text
        return GeminiAssistantMessage(response)
